# Home

## Why MMTF-14K?

In recent years, we have witnessed an unprecedented explosion of videos content created, shared, and consumed through various web channels. For example, according to the statistics, as of 2016, more than 500 hours of video was uploaded to YouTube every minute; this is equivalent to say if one would like to watch all the videos uploaded in one hour, it will take a genuine time of more than 3 years of continuous watching to see them all! As a consequence, it has become harder and harder for the users to find interesting new content using traditional forms of video search. As a countermeasure, recommender systems (RS) that automatically determine content that a user may like have emerged and evolved during the last decade.

There is an obvious need for researchers and practitioners to have access to stable, large-scale, and multimodal datasets of movies to research personalization, search/retrieval, and recommender systems in the domain. Past efforts to establish such datasets includes the MovieLens (ML) dataset which contains timestamped preference information of users for movies (on a 5-point Likert scale), in order to facilitate research on personalized movie search and recommendation. However, one frequently expressed concern about such datasets is related to the lack of real content features, which describe audio and visual properties of the movies. In fact, while in the multimedia retrieval community, content descriptors extracted from the audio-visual channel have been researched intensely, the recommender systems community interpreted for a long time the term ``content'' to refer to metadata only. 

In this vein, we provide a new dataset and benchmark MMTF-14K, specifically designed for assessing for recommender system research ...



